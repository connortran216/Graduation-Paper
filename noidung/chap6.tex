\chapter{Implementation and Evaluation}
\section{Training Mask R-CNN}
\noindent
	
	In this research, we take advantage of a pre-trained model of Mask R-CNN and fine-tune it to for our problem instead of training the whole architecture of Mask R-CNN from scratch due to the lack of time and resources. Although Facebook's researchers published the official source code in 2018, we prefer using the unofficial one from Matterport Inc which was published in 2017 due to the majority of deep learning practitioner community (5k3 fork times compared to 9k6 ones, respectively). Further information, you can access the source code via this link \url{https://github.com/matterport/Mask_RCNN}.
	
	In addition, the whole training process was executed on Google's CoLab to take advantages of GPU power (which we did not have). Besides, the training loss and the validation loss, we also calculated mAP and mAR to evaluate the models more precisely.
	
\subsection{Configuration setting}
\noindent

	In the training phrase, focused on fine-tune several hyper-parameters that affected strongly for transfer learning technique. According to \cite{maskrcnn}, the default configuration was set based on Faster R-CNN due to its robustness on not only on detection task but also segmentation one. Look at Table \ref{table:hyperparameters} for more details about hyper-parameters.
	
	\begin{table}[H]
		\begin{tabularx}{1\textwidth} {
				| >{\raggedright\arraybackslash}X 
				| >{\raggedright\arraybackslash}X
				| >{\raggedright\arraybackslash}X 
				| >{\raggedright\arraybackslash}X
				| >{\raggedright\arraybackslash}X
				| >{\raggedright\arraybackslash}X
				| >{\raggedright\arraybackslash}X
				| >{\raggedright\arraybackslash}X
				| >{\raggedright\arraybackslash}X
				| >{\raggedright\arraybackslash}X
				| >{\raggedright\arraybackslash}X
				| >{\raggedright\arraybackslash}X
				| >{\raggedright\arraybackslash}X
				| >{\raggedright\arraybackslash}X  
				| >{\raggedright\arraybackslash}X | }
			\hline
			Model & Describe \\
			\hline
			IMAGE RESIZE MODE & Input image resizing. \\
			\hline
			STEPS PER EPOCH & Number of training steps per epoch. Don't set this too small to avoid spending a lot of time on validation stats. \\
			\hline
			VALIDATION STEPS & Number of validation steps to run at the end of every training epoch. A bigger number improves accuracy of validation stats, but slows down the training. \\
			\hline
			BACKBONE & Backbone network architecture. ResNet50 and ResNet101 are supported. \\
			\hline
			LEARNING RATE & Learning rate. \\
			\hline
			LEARNING MOMENTUM & Momentum \\
			\hline
			OPTIM & Optimization algorithm. \\
			\hline
			WEIGHT DECAY & Weight decay regularization. \\
			\hline
			LOSS WEIGHTS & Loss weights for more precise optimization. (Focus on desired loss) \\
			\hline
			
		\end{tabularx}
		\caption{Hyper-parameter description}
		\label{table:hyperparameters}
		
	\end{table}
	
	In Table \ref{table:hyperparameters}, we only list several hyper-parameters that are important in the fine-tune process. In addition, for more information of hyper-parameters, we recommend reader to read at \url{https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/config.py}.
	
\subsection{Training}
\label{subsection:mrcnn_training}
\noindent

	As mentioned before, we used transfer learning technique to train Mask R-CNN. The dataset we used was described in \ref{subsection:dataset}. With a small pre-processing, we changed the annotation images and exported annotation's information (COCO's format) from it to fit Mask R-CNN's input requirement. In addition, we also applied data augmentation to increase the dataset size in order to avoid over-fitting problem. To be more precise, we created more images by:
	
	\begin{itemize}
		\item Flipping 50\% images vertically and horizontally, respectively.
		\item Rotating all images by 90, 180, and 270 degree.
		\item Multiplying images by a random value sampled uniformly from the interval [0.8, 1.5], making some images darker and others brighter.
		\item Blurring images using a gaussian kernel with a random standard deviation sampled uniformly (per image) from the interval [0.0, 5.0].
	\end{itemize}
	
	In the training process, we took advantages by using a pre-trained of Mask R-CNN trained on COCO dataset.... INTRODUCE the SCORE. Because we only trained the head of the network, we decided to use 20-30 epochs.
	
	
	The result is illustrated in Table \ref{table:exp1}, \ref{table:exp2}. All of these configurations were set and fin-tuned based on \cite{maskrcnn} and \url{https://github.com/matterport/Mask_RCNN}.
	
	% Please add the following required packages to your document preamble:
	% \usepackage{multirow}
	% \usepackage{longtable}
	% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly

	\begin{longtable}[c]{|c|l|l|l|l|l|}
		\hline
		& \multicolumn{1}{c|}{\textbf{Test 1}} & \multicolumn{1}{c|}{\textbf{Test 2}} & \multicolumn{1}{c|}{\textbf{Test 3}} & \multicolumn{1}{c|}{\textbf{Test 4}} & \multicolumn{1}{c|}{\textbf{Test 5}} \\ \hline
		\endfirsthead
		%
		\multicolumn{6}{c}%
		{{\bfseries Table \thetable\ continued from previous page}} \\
		\endhead
		%
		\textbf{Backbone} & Resnet50 & Resnet50 & Resnet101 & Resnet101 & Resnet101 \\ \hline
		\textbf{Optimizer} & SGD & Adam & SGD & Adam & SGD \\ \hline
		\textbf{Learning rate} & 0.001 & 0.001 & 0.001 & 0.001 & 0.0001 \\ \hline
		\textbf{Weight decay} & 0.0005 & 0.0005 & 0.0005 & 0.0005 & 0.0005 \\ \hline
		\textbf{Momentum} & 0.9 & 0.9 & 0.9 & 0.9 & 0.9 \\ \hline
		\textbf{Epoch} & 20 & 20 & 20 & 20 & 30 \\ \hline
		\textbf{Train - Val step} & 150 - 50 & 150 - 50 & 150 - 50 & 150 - 50 & 150 - 50 \\ \hline
		\textbf{Detection min confidence} & 0 & 0 & 0.9 & 0.9 & 0.9 \\ \hline
		\textbf{Detection NMS threshold} & 0.9 & 0.9 & 0.9 & 0.9 & 0.9 \\ \hline
		\multirow{2}{*}{\textbf{RPN anchor scale}} & \multirow{2}{*}{16, 32, 64, 128, 256} & \multirow{2}{*}{16, 32, 64, 128, 256} & \multirow{2}{*}{16, 32, 64, 128, 256} & \multirow{2}{*}{16, 32, 64, 128, 256} & \multirow{2}{*}{16, 32, 64, 128, 256} \\
		&  &  &  &  &  \\ \hline
		\textbf{Resize mode} & square & square & square & square & square \\ \hline
		\multirow{6}{*}{\textbf{Loss weight}} & rpn class loss: 1.0, & rpn class loss: 1.0, & rpn class loss: 1.0, & rpn class loss: 1.0, & rpn class loss: 1.0, \\ \cline{2-6} 
		& rpn bbox loss: 1.0, & rpn bbox loss: 1.0, & rpn bbox loss: 1.0, & rpn bbox loss: 1.0, & rpn bbox loss: 1.0, \\ \cline{2-6} 
		& mrcnn class loss: 1.0, & mrcnn class loss: 1.0, & mrcnn class loss: 1.0, & mrcnn class loss: 1.0, & mrcnn class loss: 1.0, \\ \cline{2-6} 
		& mrcnn bbox loss: 1.0, & mrcnn bbox loss: 1.0, & mrcnn bbox loss: 1.0, & mrcnn bbox loss: 1.0, & mrcnn bbox loss: 1.0, \\ \cline{2-6} 
		& \multirow{2}{*}{mrcnn mask loss: 3.0} & \multirow{2}{*}{mrcnn mask loss: 3.0} & \multirow{2}{*}{mrcnn mask loss: 3.0} & \multirow{2}{*}{mrcnn mask loss: 3.0} & \multirow{2}{*}{mrcnn mask loss: 3.0} \\
		&  &  &  &  &  \\ \hline
		\textbf{mAP} & 0.511111 & 0.822222222 & 0.977777777 & 0.0 & 0.9419753 \\ \hline
		\textbf{mAR} & 0.533333 & 0.837037037 & 0.977777777 & 0.0 & 0.9703703 \\ \hline
		\textbf{NOTE} & SGD train faster but trade off accuracy & Adam shows better much results but training time is slower than SGD & SGD shows it is better in using resnet101 & Training loss went NaN value because lr is high &  \\ \hline
	
	\caption{Experiment results}
	\label{table:exp1}
	\end{longtable}

	
	% Please add the following required packages to your document preamble:
	% \usepackage{multirow}
	% \usepackage{longtable}
	% Note: It may be necessary to compile the document several times to get a multi-page table to line up properly
	\begin{longtable}[c]{|c|l|l|l|l|l|}
		\hline
		& \multicolumn{1}{c|}{\textbf{Test 6}} & \multicolumn{1}{c|}{\textbf{Test 7}} & \multicolumn{1}{c|}{\textbf{Test 8}} & \multicolumn{1}{c|}{\textbf{Test 9}} & \multicolumn{1}{c|}{\textbf{Test 10}} \\ \hline
		\endhead
		%
		\textbf{Backbone} & Resnet50 & Resnet50 & Resnet50 & Resnet101 & Resnet101 \\ \hline
		\textbf{Optimizer} & Adam & Adam & SGD & SGD & Adam \\ \hline
		\textbf{Learning rate} & 0.0001 & 0.0001 & 0.001 & 0.001 & 0.0001 \\ \hline
		\textbf{Weight decay} & 5,00E-05 & 5,00E-05 & 0.0005 & 0.0005 & 0.0005 \\ \hline
		\textbf{Momentum} & 0.9 & 0.9 & 0.9 & 0.9 & 0.9 \\ \hline
		\textbf{Epoch} & 30 & 30 & 30 & 30 & 30 \\ \hline
		\textbf{Train - Val step} & 150 - 50 & 150-50 & 150 - 50 & 150 - 50 & 150 - 50 \\ \hline
		\textbf{Detection min confidence} & 0.7 & 0.7 & 0.7 & 0.7 & 0.9 \\ \hline
		\textbf{Detection NMS threshold} & 0.7 & 0.7 & 0.7 & 0.7 & 0.9 \\ \hline
		\multirow{2}{*}{\textbf{RPN anchor scale}} & \multirow{2}{*}{16, 32, 64, 128, 256} & \multirow{2}{*}{16, 32, 64, 128, 256} & \multirow{2}{*}{16, 32, 64, 128, 256} & \multirow{2}{*}{16, 32, 64, 128, 256} & \multirow{2}{*}{16, 32, 64, 128, 256} \\
		&  &  &  &  &  \\ \hline
		\textbf{Resize mode} & square & square & square & square & square \\ \hline
		\multirow{6}{*}{\textbf{Loss weight}} & rpn class loss: 1.0, & rpn class loss: 1.0, & rpn class loss: 1.0, & rpn class loss: 1.0, & rpn class loss: 1.0, \\ \cline{2-6} 
		& rpn bbox loss: 1.0, & rpn bbox loss: 1.0, & rpn bbox loss: 1.0, & rpn bbox loss: 1.0, & rpn bbox loss: 1.0, \\ \cline{2-6} 
		& mrcnn class loss: 1.0, & mrcnn class loss: 1.0, & mrcnn class loss: 1.0, & mrcnn class loss: 1.0, & mrcnn class loss: 1.0, \\ \cline{2-6} 
		& mrcnn bbox loss: 1.0, & mrcnn bbox loss: 1.0, & mrcnn bbox loss: 1.0, & mrcnn bbox loss: 1.0, & mrcnn bbox loss: 1.0, \\ \cline{2-6} 
		& \multirow{2}{*}{mrcnn mask loss: 1.0} & \multirow{2}{*}{mrcnn mask loss: 1.0} & \multirow{2}{*}{mrcnn mask loss: 1.0} & \multirow{2}{*}{mrcnn mask loss: 1.0} & \multirow{2}{*}{mrcnn mask loss: 1.0} \\
		&  &  &  &  &  \\ \hline
		\textbf{mAP} & 0.43703703703 & 0.955555555556 & 0.16296296298 & 0.9407407408 &  \\ \hline
		\textbf{mAR} & 0.62222222222 & 0.970370370370 & 0.16296296298 & 0.9851851852 &  \\ \hline
		\textbf{NOTE} & ??? & ??? & SGD not works well with resnet50 &  &  \\ \hline
	
	\caption{Experiment results}
	\label{table:exp2}
	\end{longtable}
	
	Firstly, we started our training process with "Test 1" and "Test 2" in order to find out which optimizer was better than the other. In general, it is noticeable that Adam optimizer converged slightly faster than SGD one. Moreover, model with Adam optimizer showed a much better mAP and mAR compared to model with SGD. Yet, the total loss, especially the mask loss was still high (more than 0.4 and 0.2, respectively). Let's look at Figure \ref{fig:loss_12} and \ref{fig:mask_loss_12} for more details.
	
	\begin{figure}[H]
		\centering
		\subcaptionbox{Total loss in Test 1.}{\includegraphics[width=0.45\textwidth]{./hinhanh/chap6/loss_t1.png}}%
		\hfill % <-- Seperation
		\subcaptionbox{Total loss in Test 2.}{\includegraphics[width=0.45\textwidth]{./hinhanh/chap6/loss_t2.png}}%
		\hfill % <-- Seperation
		\caption{Comparison between Test 1 and Test 2 loss.}
		\label{fig:loss_12}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\subcaptionbox{Mask loss in Test 1.}{\includegraphics[width=0.45\textwidth]{./hinhanh/chap6/mask_loss_t1.png}}%
		\hfill % <-- Seperation
		\subcaptionbox{Mask loss in Test 2.}{\includegraphics[width=0.45\textwidth]{./hinhanh/chap6/mask_loss_t2.png}}%
		\hfill % <-- Seperation
		\caption{Comparison between Test 1 and Test 2  mask loss.}
		\label{fig:mask_loss_12}
	\end{figure}

\noindent	
	\textbf{Noted:} In training phrase, we mostly paid attention for the mask loss due to the characteristic of the problem which was instance segmentation. Therefore, we altered the weight balance with the ratio was 1.0 - 1.0 - 3.0 (for classification loss, detection loss, and segmentation loss, respectively) in order to calculate the perimeter of HC with the minimum error using the generated mask.
	
 	In next the experiments, we change the backbone from ResNet50 to ResNet101 to see could we actually improve the model by going \textit{deeper}? And the results were quite significant. In "Test 3", with ResNet101, we improved the model mAP and mAR from ~0.5 to 0.97777 which was the highest score in lots of experiments we had proceeded. However, the total loss of "Test 4" went to NaN due to the high learning rate which showed Adam optimizer was quite sensitive to the high learning rate (overshooting problem). Therefore, in "Test 5" we decreased the learning rate to 0.0001, consequently, we could boost mAP and mAR score to 0.9419753 and 0.9703703, respectively.
	
	Although, Adam optimizer with ResNet101 backbone also showed a remarkable result (Test7, 10), SGD optimizer with ResNet101 backbone illustrated the improvement in results of "Test 3" and "Test 9" with mAP and mAR are (0.977777, 0.977777) and (0.9407408, 0.9851852), respectively.
	
\subsection{Inference}
\label{subsection:mrcnn_inference}
\noindent
	
	The testing phrase was executed on the PC with hardwares as following:
	
	\begin{itemize}
		\item CPU Intel Core i5 8th.
		\item RAM 12GB.
	\end{itemize}
	
	Even with this minimum system, Mask R-CNN could generate mask with around 1~2 second for each testing image. The result was shown in Figure \ref{fig:train_test_result}.
	
	\begin{figure}[H]
		\centering
		\subcaptionbox{Generated mask on training sample.}{\includegraphics[width=0.45\textwidth]{./hinhanh/chap6/training_result.png}}%
		\hfill % <-- Seperation
		\subcaptionbox{Generated mask on testing sample.}{\includegraphics[width=0.45\textwidth]{./hinhanh/chap6/testing_result.png}}%
		\hfill % <-- Seperation
		\caption{We can see that segmentation part on testing sample were not perfect as training one. Therefore, the perimeter was estimated from this mask would be affected.}
		\label{fig:train_test_result}
	\end{figure}
	
	As we can see from Figure \ref{fig:train_test_result}, the generated mask was fit perfectly on the fetus' head. Yet, the model was not robust enough on the testing image (we can still see the white part of the skull that had not covered yet by the mask).
	
\section{Building the Restful API}
\noindent

	In this section, we will introduce our proposed system that includes five components includes 2 parts to transfer and manage the input, and 3 parts to process HC images and estimate the perimeter of the fetus' head. Each of the components is an restful API which works on specific task (inspired from micro service design). Therefore, in the future, we can easily scale up the number of workers (API) to speed up the process and manage them more conveniently in the deployment phrase. For more details, the system's design is shown in \ref{fig:hc_system}.
	
	\begin{figure}[H]
		\centering
		{\includegraphics[width=0.9\textwidth]{./hinhanh/chap6/hc_system.png}}
		\caption{Proposed demo HC estimator system.}
		\label{fig:hc_system}
	\end{figure}

\subsection{Service Gateway}
\noindent

	Briefly, service gateway is an API that receives fetus' head from multiple sources such as images, video, or even streaming form. We decided to separate it to be standalone in order to process multiple transfer types more conveniently. However, in this thesis, this system is simply a demo so it only supports image transfer which is sent through a HTTP request.
	
	More precisely, the input data for Service Gateway are pixel size (to calculate the perimeter) and image of fetus' head in RGB color channel \ref{subsection:dataset}, .
	
\subsection{Center Processing}
\noindent

	This API, in short, it is like a CPU that sends data and receives result to/from others APIs. To be more specific:
	
	\begin{itemize}
		\item Firstly, it sends a image to Mask Generator, and then receive a binary mask.
		\item Secondly, it sends the binary mask and the image to Ellipse Fitter, and then receive an ellipse that fits on the mask.
		\item Finally, the ellipse's information is processed by Perimeter estimator to calculate the circumference of the ellipse (which is the fetus' head).
	\end{itemize}
	
	Besides, it works as a manager to monitor, report, etc.. the process of others APIs. Moreover, because we plan to increase the number of workers for specific task as in Figure \ref{fig:hc_system}, it is necessary to have a center processing to control the work flow of the system.

\subsection{Mask Generator}
\noindent
	
	As the first processing stage, Mask Generator receives a request from Center Processing to segment the fetus's head in an RGB image and returns a binary mask for it. Base on previous experiments, we decide to use the weights from "Test 3" (which is our best model) to build Mask Generator API despite its imperfection in inference stage (but the result is still acceptable) as Figure \ref{fig:train_test_result} shows. 
	
	Even though the whole mask generating process is worked on a non-GPU computer, it still maintains the processing speed at 1~2s per image. Therefore, with a better hardware, we believe that the processing speed will definitely decrease and may achieve real-time processing speed to a certain extent.
	
\subsection{Ellipse Fitter}
\noindent

	After generating mask for the fetus' head, the Contour-finding and the Ellipse-specific algorithms are applied to adjust the polygon shape of the mask to the ellipse shape as in Figure \ref{fig:post_process}.
	
	\begin{figure}[H]
		\centering
		\subcaptionbox{The binary mask.}{\includegraphics[width=0.45\textwidth]{./hinhanh/chap6/crop_mask.jpg}}%
		\hfill % <-- Seperation
		\subcaptionbox{The ellipse boundary on the fetus' head.}{\includegraphics[width=0.45\textwidth]{./hinhanh/chap6/ellipse_on_crop_mask.jpg}}%
		\hfill % <-- Seperation
		\caption{Combine finding contour algorithm with ellipse fitting one to  it an ellipse on the image.}
		\label{fig:post_process}
	\end{figure}

	Precisely, a curve joining of all continuous points (along the boundary of the binary mask) is found by using algorithm from \cite{findContours}. Despite this method is simple to implement through OpenCV, it works quite efficiently on our binary mask.
	
	
	For an ellipse, its function requires a set $ \chi = (x^2, xy, y^2, x, y, 1)\ $. However, we only need five parameters to define an ellipse on 2D space. 
	
	\begin{itemize}
		\item (x, y): center x, center y.
		\item (MA, ma): semi axes a, semi axes b.
		\item Angle radian
	\end{itemize}
	
	To achieve this goal, we fit the ellipse around this set of 2D points by employing the Direct Least Square approach from \cite{direct_ellipse}.
	
\subsection{Perimeter Estimator}
\noindent
	
	After receiving the ellipse's coordinates from Ellipse Fitter API, we estimate the ellipse's perimeter by employing Ramanujan’s formula \cite{perimeter}. The approximation of Ramanujan is demonstrated as in Equation \eqref{eq:perimeter}:
	
	\[ h = \frac{(a-b)^2}{(a+b)^2} \]
	
	\[ p \approx \pi (a+b) (1 + \frac{3h}{(10 + \sqrt{4 - 3h})} ) \label{eq:perimeter} \]

\noindent
	Then we multiply the $p$ with the pixel size to scale the result to the extract measure.

		
	
	